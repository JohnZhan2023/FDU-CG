{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Some useful resources:  \n",
    "* [huggingface](https://huggingface.co/docs/diffusers/v0.13.0/en/training/text2image)  \n",
    "* [training-example](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "!pip install diffusers\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, UNet2DConditionModel, AutoencoderKL, DDPMScheduler \n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.utils import ContextManagers\n",
    "!pip install accelerate\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Module\n",
    "Modified from this [example](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_id_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "resolution = 512\n",
    "weight_dtype=torch.float16  # datatype\n",
    "revision = None  # Revision of pretrained model identifier from huggingface.co/models.\n",
    "variant = None  # Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\",\n",
    "\n",
    "# Components\n",
    "unet = UNet2DConditionModel.from_pretrained(module_id_or_path, subfolder='unet')\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    module_id_or_path, \n",
    "    subfolder=\"vae\", \n",
    "    revision=revision, \n",
    "    variant=variant\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    module_id_or_path, \n",
    "    subfolder=\"text_encoder\", \n",
    "    revision=revision, \n",
    "    variant=variant\n",
    ")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(module_id_or_path, subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    module_id_or_path, \n",
    "    subfolder=\"tokenizer\", \n",
    "    revision=revision\n",
    ")\n",
    "safety_checker = None\n",
    "feature_extractor = ...\n",
    "\n",
    "# build the pipe\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    module_id_or_path,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    safety_checker=safety_checker,\n",
    "    revision=revision,\n",
    "    variant=variant,\n",
    "    torch_dtype=weight_dtype,\n",
    ")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module Setup\n",
    "Here I used one task to make sure the module is setup correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "#response = requests.get(url)\n",
    "# init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "# init_image = init_image.resize((768, 512))\n",
    "\n",
    "init_image = Image.open('example.png').convert('RGB')\n",
    "c, r = init_image.size\n",
    "c = c // 3\n",
    "init_image = init_image.crop((0, 0, c, r))\n",
    "init_image = init_image.resize((768, 512))\n",
    "\n",
    "#prompt = \"other views of the street\"\n",
    "#prompt = \"face left\"\n",
    "prompt=\"face left side of the road\"\n",
    "\n",
    "images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n",
    "images[0].save(\"fantasy_landscape.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the Module\n",
    "(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel, compute_dream_and_update_latents, compute_snr\n",
    "\n",
    "# Freeze vae and text_encoder and set unet to trainable\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.train()\n",
    "\n",
    "# training dataset\n",
    "train_dataloader = ...\n",
    "\n",
    "# Hyper params\n",
    "max_train_steps=15000 \n",
    "learning_rate=1e-05\n",
    "max_grad_norm=1\n",
    "train_batch_size=1\n",
    "optimizer_cls = torch.optim.AdamW\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay=1e-2\n",
    "adam_epsilon=1e-8\n",
    "num_train_epochs = 100\n",
    "noise_offset=0\n",
    "input_perturbation = 0\n",
    "prediction_type=None\n",
    "snr_gamma=None   # recommend 5.0, if you want to use it.\n",
    "gradient_accumulation_steps=1\n",
    "report_to=\"tensorboard\"\n",
    "mixed_precision=None\n",
    "output_dir=\"sd-model-finetuned\"\n",
    "logging_dir=\"logs\"\n",
    "accelerator_project_config=ProjectConfiguration(project_dir=output_dir, logging_dir=logging_dir)\n",
    "lr_scheduler='constant' # must be one of [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
    "            #           ' \"constant\", \"constant_with_warmup\"]'\n",
    "lr_warmup_steps=500\n",
    "\n",
    "# optimizer\n",
    "optimizer = optimizer_cls(\n",
    "    unet.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay,\n",
    "    eps=adam_epsilon,\n",
    ")\n",
    "\n",
    "# accelerator\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    mixed_precision=mixed_precision,\n",
    "    log_with=report_to,\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "lr_scheduler = get_scheduler(\n",
    "    lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=max_train_steps * accelerator.num_processes,\n",
    ")\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    unet, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about loading custom images [here](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and update dataloader.\n",
    "import datasets\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset_name = \"lambdalabs/pokemon-blip-captions\"\n",
    "dataset_config_name=None\n",
    "cache_dir=None\n",
    "train_data_dir=None\n",
    "dataloader_num_workers=None\n",
    "center_crop=False\n",
    "random_flip=False\n",
    "image_column='image'\n",
    "caption_column='text'\n",
    "seed=None\n",
    "max_train_samples=None\n",
    "\n",
    "dataset = load_dataset(\n",
    "    dataset_name,\n",
    "    dataset_config_name,\n",
    "    cache_dir=cache_dir,\n",
    "    data_dir=train_data_dir,\n",
    ")\n",
    "DATASET_NAME_MAPPING = {\n",
    "    \"lambdalabs/naruto-blip-captions\": (\"image\", \"text\"),\n",
    "}\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "# 6. Get the column names for input/target.\n",
    "dataset_columns = DATASET_NAME_MAPPING.get(dataset_name, None)\n",
    "if image_column is None:\n",
    "    image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "else:\n",
    "    image_column = image_column\n",
    "    if image_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--image_column' value '{image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if caption_column is None:\n",
    "    caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "else:\n",
    "    caption_column = caption_column\n",
    "    if caption_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--caption_column' value '{caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    captions = []\n",
    "    for caption in examples[caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            # take a random caption if there are multiple\n",
    "            captions.append(random.choice(caption) if is_train else caption[0])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "            )\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "def preprocess_train(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    if max_train_samples is not None:\n",
    "        dataset[\"train\"] = dataset[\"train\"].shuffle(seed=seed).select(range(max_train_samples))\n",
    "    # Set the training transforms\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution),\n",
    "        transforms.RandomHorizontalFlip() if random_flip else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training(fine-tuning)\n",
    "for epoch in range(num_train_epochs):\n",
    "    train_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Convert images to latent space\n",
    "        latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        if noise_offset:\n",
    "            # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n",
    "            noise += noise_offset * torch.randn(\n",
    "                (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n",
    "            )\n",
    "        if input_perturbation:\n",
    "            new_noise = noise + input_perturbation * torch.randn_like(noise)\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        if input_perturbation:\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, new_noise, timesteps)\n",
    "        else:\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(batch[\"input_ids\"], return_dict=False)[0]\n",
    "\n",
    "        # Get the target for loss depending on the prediction type\n",
    "        if prediction_type is not None:\n",
    "            # set prediction_type of scheduler if defined\n",
    "            noise_scheduler.register_to_config(prediction_type=prediction_type)\n",
    "\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        # Predict the noise residual and compute loss\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "\n",
    "        if snr_gamma is None:\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        else:\n",
    "            # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n",
    "            # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n",
    "            # This is discussed in Section 4.2 of the same paper.\n",
    "            snr = compute_snr(noise_scheduler, timesteps)\n",
    "            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n",
    "                dim=1\n",
    "            )[0]\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                mse_loss_weights = mse_loss_weights / snr\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                mse_loss_weights = mse_loss_weights / (snr + 1)\n",
    "\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # Gather the losses across all processes for logging (if we use distributed training).\n",
    "        avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n",
    "        train_loss += avg_loss.item() / gradient_accumulation_steps\n",
    "        print(f\"training loss is {train_loss}\")\n",
    "\n",
    "        # Backpropagate\n",
    "        accelerator.backward(loss)\n",
    "        if accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the module\n",
    "Use previous cell to verify that the weights of the weights has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "init_image = init_image.resize((768, 512))\n",
    "\n",
    "prompt = \"A fantasy landscape, trending on artstation\"\n",
    "\n",
    "images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n",
    "images[0].save(\"fantasy_landscape.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
